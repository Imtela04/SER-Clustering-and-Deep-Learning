# -*- coding: utf-8 -*-
"""p2tryouts.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rFk97CJz2Oty9iJhCPb5380Aj0l3_YIj
"""

import os
import numpy as np
import pandas as pd
import librosa
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras import layers, models, callbacks
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from tqdm import tqdm
import pickle
import warnings
warnings.filterwarnings('ignore')

# Define paths - assume audio files are in the current directory or a subdirectory
# You might need to adjust these paths based on your setup
AUDIO_PATH = '/content/CREMAD'  # Default Colab content directory
SAMPLE_RATE = 16000  # Sample rate to use for all audio files
MAX_AUDIO_LENGTH = 5  # Max audio length in seconds to consider
MAX_PAD_LENGTH = SAMPLE_RATE * MAX_AUDIO_LENGTH  # Max padding length in samples

# Function to list all audio files
def list_audio_files(path, extensions=['.wav']):
    """List all audio files with specified extensions in the given path"""
    audio_files = []

    for root, dirs, files in os.walk(path):
        for file in files:
            if any(file.lower().endswith(ext) for ext in extensions):
                audio_files.append(os.path.join(root, file))

    return audio_files

# Function to extract labels from filenames for SAVEE dataset
def extract_label_from_filename(filename):
    """Extract emotion label from SAVEE dataset filename"""
    # SAVEE filename format: [speaker]_[emotion][index].wav
    # e.g., DC_a01.wav - where 'a' represents 'anger'
    basename = os.path.basename(filename)
    emotion_code = basename.split('_')[1][0]

    # SAVEE emotion codes mapping
    emotion_map = {
        'a': 'anger',
        'd': 'disgust',
        'f': 'fear',
        'h': 'happiness',
        'n': 'neutral',
        'sa': 'sadness',
        'su': 'surprise'
    }

    # Special case for 'sa' (sadness) and 'su' (surprise)
    if basename.split('_')[1].startswith('sa'):
        return 'sadness'
    elif basename.split('_')[1].startswith('su'):
        return 'surprise'

    return emotion_map.get(emotion_code, 'unknown')

# Display information about the dataset
def display_dataset_info(audio_files):
    """Display basic information about the dataset"""
    print(f"Total number of audio files: {len(audio_files)}")

    # Check file formats
    formats = {}
    for file in audio_files:
        ext = os.path.splitext(file)[1].lower()
        formats[ext] = formats.get(ext, 0) + 1

    print("\nFile formats:")
    for fmt, count in formats.items():
        print(f"{fmt}: {count} files")

    # Count emotions
    emotions = {}
    for file in audio_files:
        emotion = extract_label_from_filename(file)
        emotions[emotion] = emotions.get(emotion, 0) + 1

    print("\nEmotion distribution:")
    for emotion, count in emotions.items():
        print(f"{emotion}: {count} files")

    # Sample a few files to display their info
    if audio_files:
        print("\nSampling some files for information...")
        for file in audio_files[:3]:  # Display info for first 3 files
            try:
                y, sr = librosa.load(file, sr=None)
                duration = librosa.get_duration(y=y, sr=sr)
                print(f"\nFile: {os.path.basename(file)}")
                print(f"Emotion: {extract_label_from_filename(file)}")
                print(f"Sample rate: {sr} Hz")
                print(f"Duration: {duration:.2f} seconds")
                print(f"Number of samples: {len(y)}")
            except Exception as e:
                print(f"Error loading {file}: {e}")

# Traditional feature extraction (as a baseline)
def extract_traditional_features(file_path):
    """Extract hand-crafted audio features from a file"""
    try:
        # Load audio file with consistent sample rate
        y, sr = librosa.load(file_path, sr=SAMPLE_RATE)

        # Basic features
        features = {}

        # Duration
        features['duration'] = librosa.get_duration(y=y, sr=sr)

        # Root Mean Square Energy
        features['rmse'] = np.mean(librosa.feature.rms(y=y))

        # Zero Crossing Rate
        features['zcr'] = np.mean(librosa.feature.zero_crossing_rate(y))

        # MFCCs (Mel-Frequency Cepstral Coefficients)
        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
        for i, mfcc in enumerate(mfccs):
            features[f'mfcc_{i+1}_mean'] = np.mean(mfcc)
            features[f'mfcc_{i+1}_std'] = np.std(mfcc)

        # Spectral Centroid
        spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
        features['spectral_centroid_mean'] = np.mean(spectral_centroids)
        features['spectral_centroid_std'] = np.std(spectral_centroids)

        # Spectral Bandwidth
        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]
        features['spectral_bandwidth_mean'] = np.mean(spectral_bandwidth)
        features['spectral_bandwidth_std'] = np.std(spectral_bandwidth)

        # Spectral Rolloff
        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]
        features['rolloff_mean'] = np.mean(rolloff)
        features['rolloff_std'] = np.std(rolloff)

        # Chroma Features
        chroma = librosa.feature.chroma_stft(y=y, sr=sr)
        features['chroma_mean'] = np.mean(chroma)
        features['chroma_std'] = np.std(chroma)

        return features

    except Exception as e:
        print(f"Error extracting features from {file_path}: {e}")
        return None

# Function to prepare audio data for deep learning
def prepare_audio_data(audio_files, max_pad_len=MAX_PAD_LENGTH):
    """
    Prepare audio data for deep learning by extracting:
    1. Raw waveforms (padded/trimmed to uniform length)
    2. Mel spectrograms
    3. Labels
    """
    X_wave = []  # For raw waveforms
    X_mel = []   # For mel spectrograms
    y = []       # For labels

    for file in tqdm(audio_files, desc="Preparing audio data"):
        try:
            # Extract label
            emotion = extract_label_from_filename(file)

            # Load audio with consistent sample rate
            audio, sr = librosa.load(file, sr=SAMPLE_RATE)

            # Trim or pad audio to fixed length
            if len(audio) > max_pad_len:
                audio = audio[:max_pad_len]
            else:
                audio = np.pad(audio, (0, max_pad_len - len(audio)), 'constant')

            # Extract mel spectrogram
            mel_spec = librosa.feature.melspectrogram(
                y=audio,
                sr=SAMPLE_RATE,
                n_fft=1024,
                hop_length=512,
                n_mels=128
            )
            # Convert to decibels
            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)

            # Store data and label
            X_wave.append(audio)
            X_mel.append(mel_spec_db)
            y.append(emotion)

        except Exception as e:
            print(f"Error processing {file}: {e}")

    # Convert lists to arrays
    X_wave = np.array(X_wave)
    X_mel = np.array(X_mel)

    # Add channel dimension to mel spectrograms for CNN (fixes the shape issue)
    X_mel = np.expand_dims(X_mel, axis=-1)

    # Encode labels
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)

    return X_wave, X_mel, y_encoded, label_encoder

# Build RNN model for raw waveform
def build_rnn_waveform_model(input_shape, num_classes):
    """Build an RNN model that works on raw waveforms"""
    model = models.Sequential([
        # Reshape input to fit RNN
        layers.Reshape((input_shape[0], 1), input_shape=input_shape),

        # RNN layers
        layers.GRU(128, return_sequences=True),
        layers.Dropout(0.3),
        layers.GRU(64),
        layers.Dropout(0.3),

        # Output layer
        layers.Dense(num_classes, activation='softmax')
    ])

    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

# Build 1D CNN + RNN model for raw waveform
def build_cnn_rnn_waveform_model(input_shape, num_classes):
    """Build a model with CNN feature extraction followed by RNN for raw waveforms"""
    model = models.Sequential([
        # Reshape input to fit Conv1D
        layers.Reshape((input_shape[0], 1), input_shape=input_shape),

        # CNN layers for feature extraction
        layers.Conv1D(64, kernel_size=100, strides=4, activation='relu'),
        layers.MaxPooling1D(pool_size=4),
        layers.Conv1D(128, kernel_size=50, strides=2, activation='relu'),
        layers.MaxPooling1D(pool_size=4),
        layers.Conv1D(256, kernel_size=25, strides=2, activation='relu'),
        layers.MaxPooling1D(pool_size=4),

        # RNN layers
        layers.GRU(64, return_sequences=True),
        layers.Dropout(0.3),
        layers.GRU(32),
        layers.Dropout(0.3),

        # Dense layers
        layers.Dense(64, activation='relu'),
        layers.Dense(num_classes, activation='softmax')
    ])

    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

# Build CNN model for mel spectrograms
def build_cnn_mel_model(input_shape, num_classes):
    """Build a CNN model for mel spectrograms"""
    model = models.Sequential([
        # Input layer - shape should be (batch, height, width, channels)
        layers.Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=input_shape),
        layers.BatchNormalization(),
        layers.MaxPooling2D(pool_size=(2, 2)),

        layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling2D(pool_size=(2, 2)),

        layers.Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling2D(pool_size=(2, 2)),

        layers.Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling2D(pool_size=(2, 2)),

        # Flatten and dense layers
        layers.Flatten(),
        layers.Dense(256, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(128, activation='relu'),
        layers.Dropout(0.3),
        layers.Dense(num_classes, activation='softmax')
    ])

    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    # Print model summary for debugging
    model.summary()

    return model

# Function to extract DL features from audio using trained models
def extract_dl_features(file_path, waveform_model, mel_model):
    """Extract deep learning features from audio file using trained models"""
    try:
        # Load audio with consistent sample rate
        audio, sr = librosa.load(file_path, sr=SAMPLE_RATE)

        # Trim or pad audio to fixed length
        if len(audio) > MAX_PAD_LENGTH:
            audio = audio[:MAX_PAD_LENGTH]
        else:
            audio = np.pad(audio, (0, MAX_PAD_LENGTH - len(audio)), 'constant')

        # Extract mel spectrogram
        mel_spec = librosa.feature.melspectrogram(
            y=audio,
            sr=SAMPLE_RATE,
            n_fft=1024,
            hop_length=512,
            n_mels=128
        )
        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
        # Add channel dimension for CNN
        mel_spec_db = np.expand_dims(mel_spec_db, axis=-1)

        # Get layer outputs from waveform model (internal representations)
        waveform_input = np.expand_dims(audio, axis=0)  # Add batch dimension
        waveform_features_model = tf.keras.Model(
            inputs=waveform_model.input,
            outputs=waveform_model.layers[-2].output  # Get output of the layer before softmax
        )
        waveform_features = waveform_features_model.predict(waveform_input)

        # Get layer outputs from mel spectrogram model
        mel_input = np.expand_dims(mel_spec_db, axis=0)  # Add batch dimension
        mel_features_model = tf.keras.Model(
            inputs=mel_model.input,
            outputs=mel_model.layers[-2].output  # Get output of the layer before softmax
        )
        mel_features = mel_features_model.predict(mel_input)

        # Combine features
        combined_features = np.concatenate([waveform_features.flatten(), mel_features.flatten()])

        # Create a dictionary of features
        feature_dict = {
            'file_path': file_path,
            'file_name': os.path.basename(file_path),
            'emotion': extract_label_from_filename(file_path)
        }

        # Add DL features to dictionary
        for i, feat in enumerate(waveform_features.flatten()):
            feature_dict[f'wave_dl_feat_{i}'] = feat

        for i, feat in enumerate(mel_features.flatten()):
            feature_dict[f'mel_dl_feat_{i}'] = feat

        return feature_dict

    except Exception as e:
        print(f"Error extracting DL features from {file_path}: {e}")
        return None

# Function to extract features from all files using DL models
def extract_all_dl_features(audio_files, waveform_model, mel_model):
    """Extract DL features from all audio files using trained models"""
    dl_features_list = []

    for file in tqdm(audio_files, desc="Extracting DL features"):
        features = extract_dl_features(file, waveform_model, mel_model)
        if features:
            dl_features_list.append(features)

    # Convert to DataFrame
    dl_features_df = pd.DataFrame(dl_features_list)
    return dl_features_df

# Function to visualize model training history
def plot_training_history(history):
    """Plot training history of a model"""
    plt.figure(figsize=(12, 4))

    # Plot accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Train')
    plt.plot(history.history['val_accuracy'], label='Validation')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plot loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Train')
    plt.plot(history.history['val_loss'], label='Validation')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.tight_layout()
    plt.show()

# Function to visualize audio with extracted features
def visualize_audio_with_features(file_path, dl_features=None):
    """Visualize audio waveform, spectrogram, and optionally DL features"""
    try:
        # Load audio
        y, sr = librosa.load(file_path, sr=SAMPLE_RATE)

        # Create figure
        plt.figure(figsize=(15, 10))

        # Plot waveform
        plt.subplot(3, 1, 1)
        librosa.display.waveshow(y, sr=sr)
        plt.title(f'Waveform: {os.path.basename(file_path)} - {extract_label_from_filename(file_path)}')

        # Plot spectrogram
        plt.subplot(3, 1, 2)
        D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)
        librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log')
        plt.colorbar(format='%+2.0f dB')
        plt.title('Spectrogram')

        # Plot Mel Spectrogram
        plt.subplot(3, 1, 3)
        S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)
        S_dB = librosa.power_to_db(S, ref=np.max)
        librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel')
        plt.colorbar(format='%+2.0f dB')
        plt.title('Mel Spectrogram')

        plt.tight_layout()
        plt.show()

        # If DL features are provided, visualize them
        if dl_features is not None:
            # Extract wave and mel features
            wave_features = [v for k, v in dl_features.items() if k.startswith('wave_dl_feat_')]
            mel_features = [v for k, v in dl_features.items() if k.startswith('mel_dl_feat_')]

            plt.figure(figsize=(12, 6))

            # Plot wave features
            plt.subplot(2, 1, 1)
            plt.plot(wave_features)
            plt.title('Waveform DL Features')
            plt.xlabel('Feature Index')
            plt.ylabel('Value')

            # Plot mel features
            plt.subplot(2, 1, 2)
            plt.plot(mel_features)
            plt.title('Mel Spectrogram DL Features')
            plt.xlabel('Feature Index')
            plt.ylabel('Value')

            plt.tight_layout()
            plt.show()
    except Exception as e:
        print(f"Error visualizing audio file: {e}")

# Main function
def main():
    print("Looking for audio files in the specified directory...")
    audio_files = list_audio_files(AUDIO_PATH)

    if not audio_files:
        print(f"No audio files found in {AUDIO_PATH}. Please check the path.")
        return

    # Display dataset information
    display_dataset_info(audio_files)

    # Ask to continue
    proceed = input("Do you want to proceed with deep learning feature extraction? (y/n): ")
    if proceed.lower() != 'y':
        return

    # Prepare data for deep learning
    print("\nPreparing audio data for deep learning...")
    X_wave, X_mel, y_encoded, label_encoder = prepare_audio_data(audio_files)

    print(f"Waveform data shape: {X_wave.shape}")
    print(f"Mel spectrogram data shape: {X_mel.shape}")
    print(f"Number of classes: {len(label_encoder.classes_)}")
    print("Classes:", label_encoder.classes_)

    # Split data
    X_wave_train, X_wave_test, X_mel_train, X_mel_test, y_train, y_test = train_test_split(
        X_wave, X_mel, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
    )

    # For debugging - verify shapes
    print(f"Training waveform shape: {X_wave_train.shape}")
    print(f"Testing waveform shape: {X_wave_test.shape}")
    print(f"Training mel shape: {X_mel_train.shape}")
    print(f"Testing mel shape: {X_mel_test.shape}")

    # Build and train models
    print("\nBuilding and training models...")

    # 1. CNN+RNN model for waveforms
    print("\nTraining CNN+RNN model on raw waveforms...")
    waveform_model = build_cnn_rnn_waveform_model(
        input_shape=(MAX_PAD_LENGTH,),
        num_classes=len(label_encoder.classes_)
    )

    # Create callbacks
    waveform_callbacks = [
        callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
        callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.0001)
    ]

    # Train waveform model
    waveform_history = waveform_model.fit(
        X_wave_train, y_train,
        validation_data=(X_wave_test, y_test),
        epochs=30,
        batch_size=32,
        callbacks=waveform_callbacks
    )

    # Plot training history
    plot_training_history(waveform_history)

    # 2. CNN model for mel spectrograms
    print("\nTraining CNN model on mel spectrograms...")
    mel_model = build_cnn_mel_model(
        input_shape=X_mel[0].shape,  # This should now be (128, time_steps, 1)
        num_classes=len(label_encoder.classes_)
    )

    # Create callbacks
    mel_callbacks = [
        callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
        callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.0001)
    ]

    # Train mel model
    mel_history = mel_model.fit(
        X_mel_train, y_train,
        validation_data=(X_mel_test, y_test),
        epochs=30,
        batch_size=32,
        callbacks=mel_callbacks
    )

    # Plot training history
    plot_training_history(mel_history)

    # Save models
    print("\nSaving trained models...")
    waveform_model.save('waveform_model.h5')
    mel_model.save('mel_model.h5')

    # Save label encoder
    with open('label_encoder.pkl', 'wb') as f:
        pickle.dump(label_encoder, f)

    # Extract DL features
    print("\nExtracting deep learning features...")
    dl_features_df = extract_all_dl_features(audio_files, waveform_model, mel_model)

    # Save features
    dl_features_df.to_csv('audio_dl_features.csv', index=False)
    print("Deep learning features saved to 'audio_dl_features.csv'")

    # Visualize a sample
    if audio_files:
        sample_file = audio_files[0]
        print(f"\nVisualizing sample file: {os.path.basename(sample_file)}")

        # Get DL features for the sample file
        sample_features = None
        if not dl_features_df.empty:
            matching_rows = dl_features_df[dl_features_df['file_path'] == sample_file]
            if not matching_rows.empty:
                sample_features = matching_rows.iloc[0].to_dict()

        # Visualize
        visualize_audio_with_features(sample_file, sample_features)

    print("\nFeature extraction complete!")

if __name__ == "__main__":
    main()