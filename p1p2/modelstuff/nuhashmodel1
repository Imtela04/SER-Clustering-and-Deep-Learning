import os
import numpy as np
import pandas as pd
import librosa
import tensorflow as tf
from tensorflow.keras import layers, models, Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
import re

# CHANGE THIS PATH ONLY
DATASET_PATH = r"C:\Users\imtel\OneDrive\pc2\thesis\datasets\cremad"  # e.g., r"C:\Users\imtel\OneDrive\pc2\thesis\datasets\SAVEE"

STANDARD_EMOTIONS = [
    'angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprised', 'calm', 'ps'
]

# ...existing code...
from typing import Dict

EMOTION_MAPPINGS: Dict[str, Dict[str, str]] = {
    'savee': {
        'a': 'angry', 'd': 'disgust', 'f': 'fear', 'h': 'happy',
        'n': 'neutral', 'sa': 'sad', 'su': 'surprised'
    },
    'ravdess': {
        '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',
        '05': 'angry', '06': 'fear', '07': 'disgust', '08': 'surprised'
    },
    'tess': {
        'angry': 'angry', 'disgust': 'disgust', 'fear': 'fear', 'happy': 'happy',
        'neutral': 'neutral', 'ps': 'ps', 'sad': 'sad', 'surprised': 'surprised'
    },
    'cremad': {
        'ANG': 'angry',
        'DIS': 'disgust',
        'FEA': 'fear',
        'HAP': 'happy',
        'NEU': 'neutral',
        'SAD': 'sad'
    }
}


def detect_dataset_type(dataset_path):
    """Detect dataset type from path"""
    path = dataset_path.lower()
    for key in EMOTION_MAPPINGS:
        if key in path:
            return key
    raise ValueError("Unknown dataset type in path: " + dataset_path)

def parse_emotion_from_filename(filename, dataset_type):
    dataset_type = dataset_type.lower()
    if dataset_type == 'savee':
        match = re.search(r'_([a-z]+)', filename)
        if match:
            code = match.group(1)
            return EMOTION_MAPPINGS['savee'].get(code)
    elif dataset_type == 'ravdess':
        parts = filename.split('-')
        if len(parts) > 2:
            code = parts[2]
            return EMOTION_MAPPINGS['ravdess'].get(code)
    elif dataset_type == 'tess':
        # Example: OAF_bite_neutral.wav
        parts = filename.replace('.wav', '').split('_')
        if len(parts) >= 2:
            code = parts[-1].lower()
            return EMOTION_MAPPINGS['tess'].get(code)
    elif dataset_type == 'cremad':
    # Example filename: 1001_DFA_ANG_XX.wav
        parts = filename.split('_')
        if len(parts) >= 3:
            code = parts[2].upper()
            return EMOTION_MAPPINGS['cremad'].get(code)
    return None

def load_dataset(dataset_path):
    dataset_type = detect_dataset_type(dataset_path)
    file_paths = []
    emotions = []
    for root, dirs, files in os.walk(dataset_path):
        for file in files:
            if file.endswith('.wav'):
                emotion = parse_emotion_from_filename(file, dataset_type)
                if emotion in STANDARD_EMOTIONS:
                    file_paths.append(os.path.join(root, file))
                    emotions.append(emotion)
    return file_paths, emotions

class AudioFeatureExtractor:
     """Automatic feature extraction from audio files"""

     def __init__(self, sr=22050, n_mfcc=13, n_fft=2048, hop_length=512, max_length=128):
        self.sr = sr
        self.n_mfcc = n_mfcc
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.max_length = max_length

     def extract_features(self, audio_path):
        """Extract multiple audio features automatically"""
        try:
            # Load audio file
            y, sr = librosa.load(audio_path, sr=self.sr)

            # Pad or truncate to fixed length
            target_length = self.max_length * self.hop_length
            if len(y) > target_length:
                y = y[:target_length]
            else:
                y = np.pad(y, (0, target_length - len(y)), mode='constant')

            # Extract features with fixed dimensions
            features = {}

            # MFCC features (time_steps, n_mfcc)
            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=self.n_mfcc,
                                    n_fft=self.n_fft, hop_length=self.hop_length)
            # Ensure fixed time dimension
            if mfcc.shape[1] > self.max_length:
                mfcc = mfcc[:, :self.max_length]
            elif mfcc.shape[1] < self.max_length:
                mfcc = np.pad(mfcc, ((0, 0), (0, self.max_length - mfcc.shape[1])), mode='constant')
            features['mfcc'] = mfcc.T  # Shape: (128, 13)

            # Mel-spectrogram with fixed dimensions
            mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=self.n_fft,
                                                    hop_length=self.hop_length, n_mels=80)
            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
            if mel_spec_db.shape[1] > self.max_length:
                mel_spec_db = mel_spec_db[:, :self.max_length]
            elif mel_spec_db.shape[1] < self.max_length:
                mel_spec_db = np.pad(mel_spec_db, ((0, 0), (0, self.max_length - mel_spec_db.shape[1])), mode='constant')
            features['mel_spec'] = mel_spec_db.T  # Shape: (128, 80)

            # Chroma features
            chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_fft=self.n_fft,
                                            hop_length=self.hop_length)
            if chroma.shape[1] > self.max_length:
                chroma = chroma[:, :self.max_length]
            elif chroma.shape[1] < self.max_length:
                chroma = np.pad(chroma, ((0, 0), (0, self.max_length - chroma.shape[1])), mode='constant')
            features['chroma'] = chroma.T  # Shape: (128, 12)

            return features

        except Exception as e:
            print(f"Error processing {audio_path}: {str(e)}")
            return None

class AttentionLayer(layers.Layer):
    """Custom attention mechanism"""

    def __init__(self, attention_dim=128, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)
        self.attention_dim = attention_dim

    def build(self, input_shape):
        self.W = self.add_weight(name='attention_weight',
                                shape=(input_shape[-1], self.attention_dim),
                                initializer='glorot_uniform',
                                trainable=True)
        self.b = self.add_weight(name='attention_bias',
                                shape=(self.attention_dim,),
                                initializer='zeros',
                                trainable=True)
        self.u = self.add_weight(name='attention_u',
                                shape=(self.attention_dim,),
                                initializer='glorot_uniform',
                                trainable=True)
        super(AttentionLayer, self).build(input_shape)

    def call(self, x):
        # x shape: (batch_size, time_steps, features)
        uit = tf.nn.tanh(tf.tensordot(x, self.W, axes=1) + self.b)
        ait = tf.tensordot(uit, self.u, axes=1)
        ait = tf.nn.softmax(ait, axis=1)

        # Expand dimensions for broadcasting
        ait = tf.expand_dims(ait, -1)
        weighted_input = x * ait
        output = tf.reduce_sum(weighted_input, axis=1)

        return output

    def get_config(self):
        config = super().get_config()
        config.update({'attention_dim': self.attention_dim})
        return config

class SpeechEmotionRecognitionModel:
    """Ensemble model combining CNN, GRU, and BiLSTM with attention"""

    def __init__(self, num_classes=7):
        self.num_classes = num_classes
        self.model = None

    def build_cnn_branch(self, input_shape, name):
        """Build CNN branch for processing spectral features"""
        input_layer = layers.Input(shape=input_shape, name=f'{name}_input')

        # Reshape for 2D convolution (time, features, channels)
        x = layers.Reshape((*input_shape, 1))(input_layer)

        # First CNN channel - smaller filters
        conv1_1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)
        conv1_2 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(conv1_1)
        # Adaptive pooling based on input size
        if input_shape[0] >= 4 and input_shape[1] >= 4:
            pool1 = layers.MaxPooling2D((2, 2))(conv1_2)
        else:
            pool1 = conv1_2

        # Second CNN channel - larger filters
        conv2_1 = layers.Conv2D(64, (5, 5), activation='relu', padding='same')(x)
        conv2_2 = layers.Conv2D(64, (5, 5), activation='relu', padding='same')(conv2_1)
        if input_shape[0] >= 4 and input_shape[1] >= 4:
            pool2 = layers.MaxPooling2D((2, 2))(conv2_2)
        else:
            pool2 = conv2_2

        # Concatenate channels
        merged = layers.Concatenate()([pool1, pool2])

        # Global average pooling instead of multiple pooling layers
        gap = layers.GlobalAveragePooling2D()(merged)

        # Dense layers
        dense = layers.Dense(256, activation='relu')(gap)
        dropout = layers.Dropout(0.5)(dense)

        return input_layer, dropout

    def build_rnn_branch(self, input_shape, name):
        """Build RNN branch with GRU and BiLSTM"""
        input_layer = layers.Input(shape=input_shape, name=f'{name}_input')

        # GRU with attention
        gru_out = layers.GRU(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)(input_layer)
        gru_attention = AttentionLayer(128)(gru_out)

        # BiLSTM with attention
        bilstm_out = layers.Bidirectional(
            layers.LSTM(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)
        )(input_layer)
        bilstm_attention = AttentionLayer(128)(bilstm_out)

        # Combine GRU and BiLSTM outputs
        combined = layers.Concatenate()([gru_attention, bilstm_attention])
        dense = layers.Dense(256, activation='relu')(combined)
        dropout = layers.Dropout(0.5)(dense)

        return input_layer, dropout

    def build_model(self, feature_shapes):
        """Build the complete ensemble model"""
        inputs = []
        branches = []

        # Build branches based on available features
        for feature_name, shape in feature_shapes.items():
            if feature_name in ['mel_spec', 'chroma']:
                # Use CNN for spectral features
                input_layer, output = self.build_cnn_branch(shape, feature_name)
                inputs.append(input_layer)
                branches.append(output)
            elif feature_name == 'mfcc':
                # Use RNN for sequential features
                input_layer, output = self.build_rnn_branch(shape, feature_name)
                inputs.append(input_layer)
                branches.append(output)

        if not branches:
            raise ValueError("No valid features provided")

        # Ensemble fusion
        if len(branches) > 1:
            merged = layers.Concatenate()(branches)
        else:
            merged = branches[0]

        # Final classification layers
        dense1 = layers.Dense(512, activation='relu')(merged)
        dropout1 = layers.Dropout(0.5)(dense1)

        dense2 = layers.Dense(256, activation='relu')(dropout1)
        dropout2 = layers.Dropout(0.3)(dense2)

        dense3 = layers.Dense(128, activation='relu')(dropout2)
        dropout3 = layers.Dropout(0.2)(dense3)

        # Output layer
        output = layers.Dense(self.num_classes, activation='softmax', name='emotion_output')(dropout3)

        # Create model
        self.model = Model(inputs=inputs, outputs=output)
        return self.model

    def compile_model(self, learning_rate=0.001):
        """Compile the model"""
        self.model.compile(
            optimizer=Adam(learning_rate=learning_rate),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )

    def train_model(self, X_train, y_train, X_val, y_val, epochs=20, batch_size=16):
        """Train the model"""
        callbacks = [
            EarlyStopping(patience=10, restore_best_weights=True, monitor='val_loss'),
            ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-7, monitor='val_loss')
        ]

        history = self.model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )

        return history


def prepare_data(file_paths, emotions, feature_extractor, le):
    """Prepare data for training"""
    features_list = []
    labels = []

    print("Extracting features from audio files...")
    for i, (file_path, emotion) in enumerate(zip(file_paths, emotions)):
        if i % 50 == 0:
            print(f"Processing {i+1}/{len(file_paths)} files...")

        features = feature_extractor.extract_features(file_path)
        if features is not None and all(f is not None for f in features.values()):
            features_list.append(features)
            labels.append(emotion)

    if not features_list:
        raise ValueError("No valid features extracted from audio files")

    # Convert to structured format
    feature_arrays = {}
    feature_names = list(features_list[0].keys())

    for name in feature_names:
        feature_arrays[name] = np.array([f[name] for f in features_list])

    # Encode labels to integers using the provided LabelEncoder
    labels = le.transform(labels)

    return feature_arrays, np.array(labels)

def plot_results(history, y_true, y_pred, emotion_names):
    """Plot training results and confusion matrix"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))

    # Training history
    axes[0, 0].plot(history.history['accuracy'], label='Training Accuracy')
    axes[0, 0].plot(history.history['val_accuracy'], label='Validation Accuracy')
    axes[0, 0].set_title('Model Accuracy')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Accuracy')
    axes[0, 0].legend()
    axes[0, 0].grid(True)

    axes[0, 1].plot(history.history['loss'], label='Training Loss')
    axes[0, 1].plot(history.history['val_loss'], label='Validation Loss')
    axes[0, 1].set_title('Model Loss')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Loss')
    axes[0, 1].legend()
    axes[0, 1].grid(True)

    # Only use present classes for confusion matrix and report
    present_labels = sorted(set(y_true) | set(y_pred))
    present_names = [emotion_names[i] for i in present_labels]

    # Confusion matrix
    cm = confusion_matrix(y_true, y_pred, labels=present_labels)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=present_names, yticklabels=present_names, ax=axes[1, 0])
    axes[1, 0].set_title('Confusion Matrix')
    axes[1, 0].set_xlabel('Predicted')
    axes[1, 0].set_ylabel('Actual')

    # Classification report (as text)
    report = classification_report(y_true, y_pred, labels=present_labels, target_names=present_names)
    axes[1, 1].text(0.1, 0.1, report, fontsize=10, family='monospace',
                   verticalalignment='bottom')
    axes[1, 1].set_title('Classification Report')
    axes[1, 1].axis('off')

    plt.tight_layout()
    plt.show()

def main():
    print("Speech Emotion Recognition - Universal Dataset")
    print("=" * 50)

    # Check dataset path
    if not os.path.exists(DATASET_PATH):
        print(f"âŒ Dataset path does not exist: {DATASET_PATH}")
        print("Please update DATASET_PATH with the correct dataset path.")
        return

    # Detect dataset type
    dataset_type = detect_dataset_type(DATASET_PATH)
    print(f"Detected dataset type: {dataset_type}")

    # Initialize components
    print("ğŸ”§ Initializing components...")
    feature_extractor = AudioFeatureExtractor()

    # Load dataset
    print("ğŸ“ Loading dataset...")
    file_paths, emotions = load_dataset(DATASET_PATH)
    print(f"Found {len(file_paths)} audio files")

    if len(file_paths) == 0:
        print("âŒ No audio files found! Check your dataset structure.")
        return

    # Label encoding (uniform across datasets)
    le = LabelEncoder()
    le.fit(STANDARD_EMOTIONS)
    emotion_names = list(le.classes_)

    # Show emotion distribution
    emotion_counts = pd.Series(emotions).value_counts()
    print("ğŸ“Š Emotion distribution:")
    for name in emotion_names:
        count = emotion_counts.get(name, 0)
        if count > 0:
            print(f"  {name}: {count} samples")

    # Extract features
    print("ğŸµ Extracting audio features...")
    features_dict, labels = prepare_data(file_paths, emotions, feature_extractor, le)

    print("âœ… Feature extraction completed!")
    print("Feature shapes:")
    for name, features in features_dict.items():
        print(f"  {name}: {features.shape}")

    # Prepare data splits
    print("ğŸ”€ Splitting dataset...")
    feature_names = ['mfcc', 'mel_spec', 'chroma']
    X = [features_dict[name] for name in feature_names if name in features_dict]
    feature_shapes = {name: features_dict[name].shape[1:] for name in feature_names if name in features_dict}

    indices = np.arange(len(labels))
    train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42, stratify=labels)
    train_idx, val_idx = train_test_split(train_idx, test_size=0.2, random_state=42, stratify=labels[train_idx])

    X_train = [features[train_idx] for features in X]
    X_val = [features[val_idx] for features in X]
    X_test = [features[test_idx] for features in X]

    y_train = labels[train_idx]
    y_val = labels[val_idx]
    y_test = labels[test_idx]

    print(f"Training samples: {len(y_train)}")
    print(f"Validation samples: {len(y_val)}")
    print(f"Test samples: {len(y_test)}")

    # Build model
    print("ğŸ—ï¸ Building ensemble model...")
    model = SpeechEmotionRecognitionModel(num_classes=len(STANDARD_EMOTIONS))
    model.build_model(feature_shapes)
    model.compile_model(learning_rate=0.001)

    print("ğŸ“‹ Model Summary:")
    model.model.summary()

    # Train model
    print("ğŸš€ Training model...")
    history = model.train_model(X_train, y_train, X_val, y_val, epochs=20, batch_size=16)

    # Evaluate model
    print("ğŸ“Š Evaluating model...")
    test_loss, test_accuracy = model.model.evaluate(X_test, y_test, verbose=0)
    print(f"âœ… Test Accuracy: {test_accuracy:.4f}")
    print(f"   Test Loss: {test_loss:.4f}")

    # Make predictions
    y_pred = np.argmax(model.model.predict(X_test, verbose=0), axis=1)

    # Generate detailed report
    print("\nğŸ“ˆ Classification Report:")
    # Find which classes are actually present in y_test or y_pred
    present_labels = sorted(set(y_test) | set(y_pred))
    present_names = [emotion_names[i] for i in present_labels]

    print(classification_report(
        y_test, y_pred,
        labels=present_labels,
        target_names=present_names
    ))
    # Plot results
    print("ğŸ“Š Generating plots...")
    plot_results(history, y_test, y_pred, emotion_names)

    # Save model
    model_path = f"{dataset_type}_emotion_recognition_model.h5"
    model.model.save(model_path)
    print(f"ğŸ’¾ Model saved: {model_path}")

    print("ğŸ‰ Training completed successfully!")

if __name__ == "__main__":
    main()