import os
import numpy as np
import pandas as pd
import librosa
import tensorflow as tf
from tensorflow.keras import layers, models, Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
from skfuzzy import cmeans
import skfuzzy as fuzz
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
#used grl by lowering adversarial effect and reduced loss weight
# def mixup_data(X, y, alpha=0.2):
#     '''Returns mixed inputs, pairs of targets, and lambda'''
#     if alpha > 0:
#         lam = np.random.beta(alpha, alpha)
#     else:
#         lam = 1
#     batch_size = X[0].shape[0]
#     index = np.random.permutation(batch_size)
#     mixed_X = [lam * x + (1 - lam) * x[index] for x in X]
#     y_a, y_b = y, y[index]
#     return mixed_X, y_a, y_b, lam

from tensorflow.keras.layers import Layer

class GradientReversalLayer(Layer):
    def __init__(self, lambda_=1.0, **kwargs):
        super().__init__(**kwargs)
        self.lambda_ = lambda_

    def call(self, x):
        @tf.custom_gradient
        def reverse_grad(x):
            def grad(dy):
                return -self.lambda_ * dy
            return x, grad
        return reverse_grad(x)

# Choose dataset
DATASET_NAME = "cremad"  # or "savee", "tess", "ravdess"
DATASET_PATH = r"C:\Users\imtel\OneDrive\pc2\thesis\datasets\cremad"  # update as needed


class AudioFeatureExtractor:
    """Automatic feature extraction from audio files"""
    def __init__(self, sr=22050, n_mfcc=13, n_fft=2048, hop_length=512, max_length=128):
        self.sr = sr
        self.n_mfcc = n_mfcc
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.max_length = max_length

    def extract_features(self, audio_path):
        """Extract multiple audio features automatically"""
        try:
            # Load audio file
            y, sr = librosa.load(audio_path, sr=self.sr)

            # Pad or truncate to fixed length
            target_length = self.max_length * self.hop_length
            if len(y) > target_length:
                y = y[:target_length]
            else:
                y = np.pad(y, (0, target_length - len(y)), mode='constant')

            # Extract features with fixed dimensions
            features = {}

            # MFCC features (time_steps, n_mfcc)
            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=self.n_mfcc,
                                      n_fft=self.n_fft, hop_length=self.hop_length)

            # Ensure fixed time dimension
            if mfcc.shape[1] > self.max_length:
                mfcc = mfcc[:, :self.max_length]
            elif mfcc.shape[1] < self.max_length:
                mfcc = np.pad(mfcc, ((0, 0), (0, self.max_length - mfcc.shape[1])), mode='constant')

            features['mfcc'] = mfcc.T  # Shape: (128, 13)

            # Mel-spectrogram with fixed dimensions
            mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=self.n_fft,
                                                    hop_length=self.hop_length, n_mels=80)
            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)

            if mel_spec_db.shape[1] > self.max_length:
                mel_spec_db = mel_spec_db[:, :self.max_length]
            elif mel_spec_db.shape[1] < self.max_length:
                mel_spec_db = np.pad(mel_spec_db, ((0, 0), (0, self.max_length - mel_spec_db.shape[1])), mode='constant')

            features['mel_spec'] = mel_spec_db.T  # Shape: (128, 80)

            # Chroma features
            chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_fft=self.n_fft,
                                               hop_length=self.hop_length)
            if chroma.shape[1] > self.max_length:
                chroma = chroma[:, :self.max_length]
            elif chroma.shape[1] < self.max_length:
                chroma = np.pad(chroma, ((0, 0), (0, self.max_length - chroma.shape[1])), mode='constant')

            features['chroma'] = chroma.T  # Shape: (128, 12)

            return features

        except Exception as e:
            print(f"Error processing {audio_path}: {str(e)}")
            return None

class AttentionLayer(layers.Layer):
    """Custom attention mechanism"""
    def __init__(self, attention_dim=128, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)
        self.attention_dim = attention_dim

    def build(self, input_shape):
        self.W = self.add_weight(name='attention_weight',
                               shape=(input_shape[-1], self.attention_dim),
                               initializer='glorot_uniform',
                               trainable=True)
        self.b = self.add_weight(name='attention_bias',
                               shape=(self.attention_dim,),
                               initializer='zeros',
                               trainable=True)
        self.u = self.add_weight(name='attention_u',
                               shape=(self.attention_dim,),
                               initializer='glorot_uniform',
                               trainable=True)
        super(AttentionLayer, self).build(input_shape)

    def call(self, x):
        # x shape: (batch_size, time_steps, features)
        uit = tf.nn.tanh(tf.tensordot(x, self.W, axes=1) + self.b)
        ait = tf.tensordot(uit, self.u, axes=1)
        ait = tf.nn.softmax(ait, axis=1)

        # Expand dimensions for broadcasting
        ait = tf.expand_dims(ait, -1)
        weighted_input = x * ait
        output = tf.reduce_sum(weighted_input, axis=1)

        return output

    def get_config(self):
        config = super().get_config()
        config.update({'attention_dim': self.attention_dim})
        return config

class SpeechEmotionRecognitionModel:
    """Ensemble model combining CNN, GRU, and BiLSTM with attention"""
    def __init__(self, num_classes=8):
        self.num_classes = num_classes
        self.model = None

    def build_cnn_branch(self, input_shape, name):
        """Build CNN branch for processing spectral features"""
        input_layer = layers.Input(shape=input_shape, name=f'{name}_input')

        # Reshape for 2D convolution (time, features, channels)
        x = layers.Reshape((*input_shape, 1))(input_layer)

        # First CNN channel - smaller filters
        conv1_1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)
        conv1_2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(conv1_1)

        # Adaptive pooling based on input size
        if input_shape[0] >= 4 and input_shape[1] >= 4:
            pool1 = layers.MaxPooling2D((2, 2))(conv1_2)
        else:
            pool1 = conv1_2

        # Second CNN channel - larger filters
        conv2_1 = layers.Conv2D(128, (5, 5), activation='relu', padding='same')(x)
        conv2_2 = layers.Conv2D(128, (5, 5), activation='relu', padding='same')(conv2_1)

        if input_shape[0] >= 4 and input_shape[1] >= 4:
            pool2 = layers.MaxPooling2D((2, 2))(conv2_2)
        else:
            pool2 = conv2_2

        # Concatenate channels
        merged = layers.Concatenate()([pool1, pool2])

        # Global average pooling instead of multiple pooling layers
        gap = layers.GlobalAveragePooling2D()(merged)

        # Dense layers
        dense = layers.Dense(256, activation='relu')(gap)
        dropout = layers.Dropout(0.3)(dense)

        return input_layer, dropout

    def build_rnn_branch(self, input_shape, name):
        """Build RNN branch with GRU and BiLSTM"""
        input_layer = layers.Input(shape=input_shape, name=f'{name}_input')

        # GRU with attention
        gru_out = layers.GRU(256, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(input_layer)
        gru_attention = AttentionLayer(128)(gru_out)

        # BiLSTM with attention
        bilstm_out = layers.Bidirectional(
            layers.LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)
        )(input_layer)  
        bilstm_attention = AttentionLayer(128)(bilstm_out)

        # Combine GRU and BiLSTM outputs
        combined = layers.Concatenate()([gru_attention, bilstm_attention])
        dense = layers.Dense(256, activation='relu')(combined)
        dropout = layers.Dropout(0.3)(dense)

        return input_layer, dropout

    def build_model(self, feature_shapes, num_speakers):
        """Build the complete ensemble model"""
        inputs = []
        branches = []

        # Build branches based on available features
        for feature_name, shape in feature_shapes.items():
            if feature_name in ['mel_spec', 'chroma']:
                # Use CNN for spectral features
                input_layer, output = self.build_cnn_branch(shape, feature_name)
                inputs.append(input_layer)
                branches.append(output)
            elif feature_name == 'mfcc':
                # Use RNN for sequential features
                input_layer, output = self.build_rnn_branch(shape, feature_name)
                inputs.append(input_layer)
                branches.append(output)

        if not branches:
            raise ValueError("No valid features provided")

        # Ensemble fusion
        if len(branches) > 1:
            merged = layers.Concatenate()(branches)
        else:
            merged = branches[0]

        # Final classification layers
        dense1 = layers.Dense(512, activation='relu')(merged)
        dropout1 = layers.Dropout(0.5)(dense1)

        dense2 = layers.Dense(256, activation='relu')(dropout1)
        dropout2 = layers.Dropout(0.3)(dense2)

        dense3 = layers.Dense(128, activation='relu')(dropout2)
        dropout3 = layers.Dropout(0.2)(dense3)

        grl = GradientReversalLayer(lambda_=0.2)(dropout3)  # Lower lambda for softer adversarial effect
        speaker_output = layers.Dense(num_speakers, activation='softmax', name='speaker_output')(grl)
        emotion_output = layers.Dense(self.num_classes, activation='softmax', name='emotion_output')(dropout3)
        self.model = Model(inputs=inputs, outputs=[emotion_output, speaker_output])
        return self.model


    def compile_model(self, learning_rate=0.001):
        self.model.compile(
            optimizer=Adam(learning_rate=learning_rate),
            loss={'emotion_output': 'sparse_categorical_crossentropy', 'speaker_output': 'sparse_categorical_crossentropy'},
            loss_weights={'emotion_output': 1.0, 'speaker_output': 0.1},  # Lower adversarial loss weight
            metrics={'emotion_output': 'accuracy', 'speaker_output': 'accuracy'}
        )
    def train_model(self, X_train, y_train, X_val, y_val, speaker_labels, train_idx, val_idx, epochs=20, batch_size=16):
        """Train the model with standard training (no mixup)"""
        callbacks = [
            EarlyStopping(patience=10, restore_best_weights=True, monitor='val_loss'),
            ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-7, monitor='val_loss')
        ]
        history = self.model.fit(
            X_train, {'emotion_output': y_train, 'speaker_output': speaker_labels[train_idx]},
            validation_data=(X_val, {'emotion_output': y_val, 'speaker_output': speaker_labels[val_idx]}),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )
        return history
        # """Train the model with mixup regularization"""
        # callbacks = [
        #     EarlyStopping(patience=10, restore_best_weights=True, monitor='val_loss'),
        #     ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-7, monitor='val_loss')
        # ]

        # steps_per_epoch = len(y_train) // batch_size

        # for epoch in range(epochs):
        #     print(f"\nEpoch {epoch+1}/{epochs}")
        #     for step in range(steps_per_epoch):
        #         # Get batch indices
        #         batch_indices = np.random.choice(len(y_train), batch_size, replace=False)
        #         batch_X = [x[batch_indices] for x in X_train]
        #         batch_y = y_train[batch_indices]
        #         batch_speaker = speaker_labels[train_idx][batch_indices]

        #         # Apply mixup
        #         mixed_X, y_a, y_b, lam = mixup_data(batch_X, batch_y, alpha=0.2)
        #         mixed_speaker_a, mixed_speaker_b = batch_speaker, batch_speaker[np.random.permutation(batch_size)]

        #         # Train on mixed data
        #         self.model.train_on_batch(
        #             mixed_X,
        #             {
        #                 'emotion_output': lam * y_a + (1 - lam) * y_b,
        #                 'speaker_output': lam * mixed_speaker_a + (1 - lam) * mixed_speaker_b
        #             }
        #         )

        #     # Validation at the end of each epoch
        #     self.model.evaluate(
        #         X_val,
        #         {'emotion_output': y_val, 'speaker_output': speaker_labels[val_idx]},
        #         verbose=0
        #     )
     
class FuzzyEmotionAnalyzer:
    """Fuzzy C-Means analyzer for emotion conflicts"""
    def __init__(self, emotion_names):
        self.emotion_names = emotion_names

    def analyze_emotion_conflicts(self, predictions_proba, y_true, n_clusters=3, m=2):
        """
        Analyze emotion conflicts using Fuzzy C-Means clustering

        Args:
            predictions_proba: Prediction probabilities from the model
            y_true: True emotion labels
            n_clusters: Number of clusters for FCM
            m: Fuzziness parameter
        """
        print("\n🔍 Analyzing Emotion Conflicts using Fuzzy C-Means...")
        print("=" * 60)

        # Prepare data for FCM - transpose for sklearn format
        data = predictions_proba.T

        # Apply Fuzzy C-Means clustering
        cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(
            data, n_clusters, m, error=0.005, maxiter=1000, init=None
        )

        # Get cluster membership for each sample
        cluster_membership = np.argmax(u, axis=0)

        # Calculate conflict scores for each emotion
        conflict_results = []

        for i, (true_emotion, pred_proba) in enumerate(zip(y_true, predictions_proba)):
            primary_emotion = np.argmax(pred_proba)
            primary_confidence = pred_proba[primary_emotion]

            # Sort probabilities to find secondary emotions
            sorted_indices = np.argsort(pred_proba)[::-1]
            secondary_emotion = sorted_indices[1] if len(sorted_indices) > 1 else primary_emotion
            secondary_confidence = pred_proba[secondary_emotion]

            # Calculate conflict score (uncertainty measure)
            conflict_score = 1 - primary_confidence

            # Determine if there's a significant conflict
            conflict_threshold = 0.3  # Adjustable threshold
            has_conflict = (secondary_confidence > conflict_threshold) or (conflict_score > 0.5)

            # Get fuzzy membership values
            fuzzy_memberships = u[:, i]
            dominant_cluster = cluster_membership[i]

            conflict_info = {
                'sample_id': i,
                'true_emotion': self.emotion_names[true_emotion],
                'predicted_emotion': self.emotion_names[primary_emotion],
                'primary_confidence': primary_confidence,
                'secondary_emotion': self.emotion_names[secondary_emotion],
                'secondary_confidence': secondary_confidence,
                'conflict_score': conflict_score,
                'has_conflict': has_conflict,
                'fuzzy_cluster': dominant_cluster,
                'fuzzy_memberships': fuzzy_memberships,
                'prediction_probabilities': pred_proba
            }

            conflict_results.append(conflict_info)

        # Analyze conflicts by emotion class
        self._analyze_conflicts_by_emotion(conflict_results)

        # Visualize fuzzy clustering results
        self._visualize_fuzzy_clusters(u, cluster_membership, predictions_proba, y_true)

        return conflict_results, cntr, u

    def _analyze_conflicts_by_emotion(self, conflict_results):
        """Analyze conflicts grouped by primary emotion"""
        print("\n📊 Conflict Analysis by Primary Emotion:")
        print("-" * 50)

        # Group by predicted emotion
        emotion_conflicts = {name: [] for name in self.emotion_names}

        for result in conflict_results:
            if result['has_conflict']:
                emotion_conflicts[result['predicted_emotion']].append(result)

        # Display conflicts for each emotion
        for emotion, conflicts in emotion_conflicts.items():
            if conflicts:
                print(f"\n🎭 {emotion.upper()} - Found {len(conflicts)} conflicting samples:")

                for conflict in conflicts[:5]:  # Show top 5 conflicts
                    print(f"  Sample {conflict['sample_id']}:")
                    print(f"    Primary: {conflict['predicted_emotion']} ({conflict['primary_confidence']:.3f})")
                    print(f"    Secondary: {conflict['secondary_emotion']} ({conflict['secondary_confidence']:.3f})")
                    print(f"    True: {conflict['true_emotion']}")
                    print(f"    Conflict Score: {conflict['conflict_score']:.3f}")
                    print(f"    Fuzzy Cluster: {conflict['fuzzy_cluster']}")
                    print()

                if len(conflicts) > 5:
                    print(f"    ... and {len(conflicts) - 5} more conflicts\n")
            else:
                print(f"\n✅ {emotion.upper()} - No significant conflicts detected")

    def _visualize_fuzzy_clusters(self, u, cluster_membership, predictions_proba, y_true):
        """Visualize fuzzy clustering results"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # Plot 1: Fuzzy membership values
        im1 = axes[0, 0].imshow(u, aspect='auto', cmap='viridis')
        axes[0, 0].set_title('Fuzzy Membership Matrix')
        axes[0, 0].set_xlabel('Samples')
        axes[0, 0].set_ylabel('Clusters')
        plt.colorbar(im1, ax=axes[0, 0])

        # Plot 2: Cluster assignments vs true emotions
        scatter = axes[0, 1].scatter(y_true, cluster_membership,
                                   c=cluster_membership, cmap='tab10', alpha=0.6)
        axes[0, 1].set_title('Fuzzy Clusters vs True Emotions')
        axes[0, 1].set_xlabel('True Emotion Labels')
        axes[0, 1].set_ylabel('Fuzzy Cluster Assignment')
        axes[0, 1].set_xticks(range(len(self.emotion_names)))
        axes[0, 1].set_xticklabels(self.emotion_names, rotation=45)

        # Plot 3: Prediction confidence distribution
        max_confidences = np.max(predictions_proba, axis=1)
        axes[1, 0].hist(max_confidences, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
        axes[1, 0].set_title('Distribution of Maximum Prediction Confidence')
        axes[1, 0].set_xlabel('Confidence Score')
        axes[1, 0].set_ylabel('Frequency')
        axes[1, 0].axvline(x=0.7, color='red', linestyle='--', label='High Confidence Threshold')
        axes[1, 0].legend()

        # Plot 4: Conflict heatmap
        conflict_matrix = np.zeros((len(self.emotion_names), len(self.emotion_names)))

        for i, (true_label, pred_proba) in enumerate(zip(y_true, predictions_proba)):
            primary_pred = np.argmax(pred_proba)
            secondary_pred = np.argsort(pred_proba)[-2]

            if pred_proba[secondary_pred] > 0.2:  # Significant secondary prediction
                conflict_matrix[primary_pred, secondary_pred] += 1

        sns.heatmap(conflict_matrix, annot=True, fmt='.0f', cmap='Reds',
                   xticklabels=self.emotion_names, yticklabels=self.emotion_names,
                   ax=axes[1, 1])
        axes[1, 1].set_title('Emotion Conflict Heatmap')
        axes[1, 1].set_xlabel('Secondary Emotion (Conflict)')
        axes[1, 1].set_ylabel('Primary Emotion (Main Prediction)')

        plt.tight_layout()
        plt.show()

class UniversalDatasetLoader:
    """Universal loader for SAVEE, CREMA-D, TESS, and RAVDESS datasets"""
    def __init__(self, dataset_path, dataset_name):
        self.dataset_path = dataset_path
        self.dataset_name = dataset_name.lower()
        # Standardized emotion labels
        self.emotion_labels = {
            'anger': 0, 'angry': 0, 'a': 0, 'ang': 0,
            'disgust': 1, 'd': 1, 'dis': 1,
            'fear': 2, 'f': 2, 'fea':2,
            'happy': 3, 'h': 3, 'hap': 3,
            'neutral': 4, 'n': 4, 'neu': 4,
            'sad': 5, 'sadness': 5, 'sa': 5,
            'surprise': 6, 'su': 6,
            'calm': 7, 'c': 7
        }

    def load_data(self):
        file_paths, emotions, speakers = [], [], []

        if self.dataset_name == 'savee':
            for root, dirs, files in os.walk(self.dataset_path):
                for file in files:
                    if file.endswith('.wav'):
                        file_path = os.path.join(root, file)
                        filename = os.path.basename(file)
                        parts = filename.split('_')
                        if len(parts) >= 2:
                            emotion_part = parts[1].split('.')[0]
                            speaker_id = parts[0]
                            emotion_code = None
                            if emotion_part.startswith('sa'):
                                emotion_code = 'sa'
                            elif emotion_part.startswith('su'):
                                emotion_code = 'su'
                            elif len(emotion_part) > 0:
                                emotion_code = emotion_part[0]
                            if emotion_code and emotion_code in self.emotion_labels:
                                file_paths.append(file_path)
                                emotions.append(self.emotion_labels[emotion_code])
                                speakers.append(speaker_id)

        elif self.dataset_name == 'cremad':
            for root, dirs, files in os.walk(self.dataset_path):
                for file in files:
                    if file.endswith('.wav'):
                        file_path = os.path.join(root, file)
                        filename = os.path.basename(file)
                        parts = filename.split('_')
                        if len(parts) >= 3:
                            speaker_id = parts[0]
                            emotion_code = parts[2].lower()#.split('.')[0].lower()
                            if emotion_code in self.emotion_labels:
                                file_paths.append(file_path)
                                emotions.append(self.emotion_labels[emotion_code])
                                speakers.append(speaker_id)

        elif self.dataset_name == 'tess':
            for root, dirs, files in os.walk(self.dataset_path):
                for file in files:
                    if file.endswith('.wav'):
                        file_path = os.path.join(root, file)
                        speaker_id = os.path.basename(root)
                        emotion_code = file.split('_')[-1].split('.')[0].lower()
                        if emotion_code in self.emotion_labels:
                            file_paths.append(file_path)
                            emotions.append(self.emotion_labels[emotion_code])
                            speakers.append(speaker_id)

        elif self.dataset_name == 'ravdess':
            for root, dirs, files in os.walk(self.dataset_path):
                for file in files:
                    if file.lower().endswith('.wav'):
                        file_path = os.path.join(root, file)
                        filename = os.path.basename(file)
                        parts = filename.split('-')
                        if len(parts) >= 7:
                            emotion_id = int(parts[2])
                            speaker_id = parts[6].split('.')[0]  # <-- FIXED: actor ID is the 7th part
                            # RAVDESS emotion mapping (7 classes)
                            ravdess_map = {
                                1: 4,  # neutral
                                2: 7,  # calm (use 7 for calm, or map to neutral if you want only 7 classes)
                                3: 3,  # happy
                                4: 5,  # sad
                                5: 0,  # angry
                                6: 2,  # fearful
                                7: 1,  # disgust
                                8: 6   # surprised
                            }
                            if emotion_id in ravdess_map:
                                mapped_emotion = ravdess_map[emotion_id]                                
                                file_paths.append(file_path)
                                emotions.append(mapped_emotion)
                                speakers.append(speaker_id)

        else:
            raise ValueError(f"Unknown dataset name: {self.dataset_name}")

        return file_paths, emotions, speakers

def prepare_data(file_paths, emotions, feature_extractor):
    """Prepare data for training"""
    features_list = []
    labels = []

    print("Extracting features from audio files...")
    for i, (file_path, emotion) in enumerate(zip(file_paths, emotions)):
        if i % 50 == 0:
            print(f"Processing {i+1}/{len(file_paths)} files...")

        features = feature_extractor.extract_features(file_path)
        if features is not None and all(f is not None for f in features.values()):
            features_list.append(features)
            labels.append(emotion)

    if not features_list:
        raise ValueError("No valid features extracted from audio files")

    # Convert to structured format
    feature_arrays = {}
    feature_names = list(features_list[0].keys())

    for name in feature_names:
        feature_arrays[name] = np.array([f[name] for f in features_list])

    return feature_arrays, np.array(labels)

def plot_results(history, y_true, y_pred, emotion_names):
    """Plot training results and confusion matrix"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    #commented out as history is not returned in current training method(mixup)
    # Training history
    axes[0, 0].plot(history.history['emotion_output_accuracy'], label='Emotion Training Accuracy')
    axes[0, 0].plot(history.history['val_emotion_output_accuracy'], label='Emotion Validation Accuracy')
    axes[0, 0].set_title('Emotion Model Accuracy')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Accuracy')
    axes[0, 0].legend()
    axes[0, 0].grid(True)

    axes[0, 1].plot(history.history['emotion_output_loss'], label='Emotion Training Loss')
    axes[0, 1].plot(history.history['val_emotion_output_loss'], label='Emotion Validation Loss')
    axes[0, 1].set_title('Emotion Model Loss')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Loss')
    axes[0, 1].legend()
    axes[0, 1].grid(True)

    # Confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
               xticklabels=emotion_names, yticklabels=emotion_names, ax=axes[1, 0])
    axes[1, 0].set_title('Confusion Matrix')
    axes[1, 0].set_xlabel('Predicted')
    axes[1, 0].set_ylabel('Actual')

    # Classification report (as text)
    report = classification_report(
        y_true, y_pred, target_names=emotion_names, labels=list(range(len(emotion_names)))
    )
    axes[1, 1].text(0.1, 0.1, report, fontsize=10, family='monospace',
                   verticalalignment='bottom')
    axes[1, 1].set_title('Classification Report')
    axes[1, 1].axis('off')

    plt.tight_layout()
    plt.show()

def main():
    """Main training pipeline with Fuzzy C-Means analysis"""
    print(f"Speech Emotion Recognition with Fuzzy Conflict Analysis - {DATASET_NAME.upper()} Dataset")
    print("=" * 80)

    # Check dataset path
    if not os.path.exists(DATASET_PATH):
        print(f"❌ Dataset path does not exist: {DATASET_PATH}")
        print("Please update line 17 with the correct SAVEE dataset path.")
        return

    # Initialize components
    print("🔧 Initializing components...")
    dataset_loader = UniversalDatasetLoader(DATASET_PATH, DATASET_NAME)
    feature_extractor = AudioFeatureExtractor()

    # Load dataset
    print(f"📁 Loading {DATASET_NAME.upper()} dataset...")
    file_paths, emotions, speakers = dataset_loader.load_data()  # <-- Make sure load_data returns speakers!
    print(f"Found {len(file_paths)} audio files")

    if len(file_paths) == 0:
        print("❌ No audio files found! Check your dataset structure.")
        return
    # Step 2: Encode Speaker IDs
    from sklearn.preprocessing import LabelEncoder
    speaker_encoder = LabelEncoder()
    speaker_labels = speaker_encoder.fit_transform(speakers)
    num_speakers = len(np.unique(speaker_labels))

    # Show emotion distribution
    emotion_names = ['Anger', 'Disgust', 'Fear', 'Happiness', 'Neutral', 'Sadness', 'Surprise', 'Calm']
    emotion_counts = np.bincount(emotions)
    print("📊 Emotion distribution:")
    for i, (name, count) in enumerate(zip(emotion_names, emotion_counts)):
        if count > 0:
            print(f"  {name}: {count} samples")

    # Extract features
    print("🎵 Extracting audio features...")
    features_dict, labels = prepare_data(file_paths, emotions, feature_extractor)
    print("✅ Feature extraction completed!")
    print("Feature shapes:")
    for name, features in features_dict.items():
        print(f"  {name}: {features.shape}")

    # Prepare data splits
    print("🔄 Splitting dataset...")
    # Create feature list in consistent order
    feature_names = ['mfcc', 'mel_spec', 'chroma']
    X = [features_dict[name] for name in feature_names if name in features_dict]
    feature_shapes = {name: features_dict[name].shape[1:] for name in feature_names if name in features_dict}

    # Split data
    indices = np.arange(len(labels))
    train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42, stratify=labels)
    train_idx, val_idx = train_test_split(train_idx, test_size=0.2, random_state=42, stratify=labels[train_idx])

    X_train = [features[train_idx] for features in X]
    X_val = [features[val_idx] for features in X]
    X_test = [features[test_idx] for features in X]
    y_train = labels[train_idx]
    y_val = labels[val_idx]
    y_test = labels[test_idx]

    print(f"Training samples: {len(y_train)}")
    print(f"Validation samples: {len(y_val)}")
    print(f"Test samples: {len(y_test)}")

    # Build model
    print("🏗️ Building ensemble model...")
    model = SpeechEmotionRecognitionModel(num_classes=8)
    model.build_model(feature_shapes, num_speakers)
    model.compile_model(learning_rate=0.001)

    print("📋 Model Summary:")
    model.model.summary()

    # Train model
    print("🚀 Training model...")
    history = model.train_model(
        X_train, y_train, X_val, y_val, speaker_labels, train_idx, val_idx, epochs=20, batch_size=16
    )

    # Evaluate model
    print("📊 Evaluating model...")
    results = model.model.evaluate(
        X_test,
        {'emotion_output': y_test, 'speaker_output': speaker_labels[test_idx]},
        verbose=0
    )
    print("Evaluation results:", results)  # <-- Add this line

    # Now unpack the correct number of values based on the printed output
    test_loss, emotion_loss, speaker_loss, test_emotion_acc, test_speaker_acc = results
    print(f"✅ Test Emotion Accuracy: {test_emotion_acc:.4f}")
    print(f"✅ Test Speaker Accuracy: {test_speaker_acc:.4f}")
    print(f"   Test Loss: {test_loss:.4f}")

    # Make predictions with probabilities
    print("🔮 Making predictions...")
    predictions_proba = model.model.predict(X_test, verbose=0)
    emotion_pred_proba = predictions_proba[0]  # First output is emotion probabilities
    y_pred = np.argmax(emotion_pred_proba, axis=1)

    # Generate detailed report
    print("\n📋 Classification Report:")
    print(classification_report(
        y_test, y_pred, target_names=emotion_names, labels=list(range(len(emotion_names)))
    ))

    # Initialize Fuzzy Emotion Analyzer
    fuzzy_analyzer = FuzzyEmotionAnalyzer(emotion_names)

    # Analyze emotion conflicts using Fuzzy C-Means
    conflict_results, cluster_centers, fuzzy_matrix = fuzzy_analyzer.analyze_emotion_conflicts(
        emotion_pred_proba, y_test, n_clusters=3, m=2
    )
    # Plot traditional results
    print("📈 Generating plots...")
    plot_results(history, y_test, y_pred, emotion_names)

    # Additional conflict analysis summary
    print("\n" + "="*80)
    print("🎯 FUZZY CONFLICT ANALYSIS SUMMARY")
    print("="*80)

    total_conflicts = sum(1 for result in conflict_results if result['has_conflict'])
    total_samples = len(conflict_results)
    conflict_percentage = (total_conflicts / total_samples) * 100

    print(f"Total samples analyzed: {total_samples}")
    print(f"Samples with conflicts: {total_conflicts}")
    print(f"Conflict rate: {conflict_percentage:.2f}%")

    # Show most conflicted emotions
    conflict_by_emotion = {}
    for result in conflict_results:
        if result['has_conflict']:
            primary = result['predicted_emotion']
            if primary not in conflict_by_emotion:
                conflict_by_emotion[primary] = []
            conflict_by_emotion[primary].append(result)

    print("\n🔍 Most Conflicted Emotions:")
    sorted_conflicts = sorted(conflict_by_emotion.items(),
                            key=lambda x: len(x[1]), reverse=True)

    for emotion, conflicts in sorted_conflicts:
        avg_conflict_score = np.mean([c['conflict_score'] for c in conflicts])
        print(f"  {emotion}: {len(conflicts)} conflicts (avg score: {avg_conflict_score:.3f})")

    # Save model
    model_path = f'{DATASET_NAME.lower()}_emotion_recognition_model.h5'
    model.model.save(model_path)
    print(f"\n💾 Model saved: {model_path}")
    print("🎉 Training and analysis completed successfully!")

if __name__ == "__main__":
    main()